{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn import random_projection\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(0)\n",
    "import platform"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NDRindex implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def get_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Computes the norm of the vector between x1 and x2\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(x2-x1)\n",
    "\n",
    "def get_distances_lower_quartile(dataset):\n",
    "    \"\"\"\n",
    "    Determines the lower quartile of the distances\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for x1 in range(dataset.shape[0]-1):\n",
    "        for x2 in range(x1+1, dataset.shape[0]):\n",
    "            distances.append(get_distance(dataset[x1], dataset[x2]))\n",
    "    if len(distances)%4 == 0:\n",
    "        return sorted(distances)[len(distances)//4]\n",
    "    else:\n",
    "        return sum(sorted(distances)[len(distances)//4:len(distances)//4+1])/2\n",
    "\n",
    "def get_distances_matrix(dataset):\n",
    "    distances = np.zeros((dataset.shape[0], dataset.shape[0]))\n",
    "    for x1 in range(dataset.shape[0]):\n",
    "        for x2 in range(dataset.shape[0]):\n",
    "            distances[x1][x2] = get_distance(dataset[x1], dataset[x2])\n",
    "    return distances\n",
    "\n",
    "def get_average_scale(dataset):\n",
    "    return get_distances_lower_quartile(dataset)/(np.log10(dataset.shape[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def get_geometric_center(cluster, dim=2):\n",
    "    \"\"\"\n",
    "    Cluster is a list of points.\n",
    "    The geometric center is the mass center of the cluster, all the points having a mass of 1.\n",
    "    \"\"\"\n",
    "    return [np.sum(np.array(cluster)[:, i])/len(cluster) for i in range(dim)]\n",
    "\n",
    "def fill_clusters(dataset, dim=2):\n",
    "    K = 0\n",
    "    clusters = [[[], [0 for _ in range(dim)]]]\n",
    "\n",
    "    # Shuffle the points to access them in a random order\n",
    "    points = np.arange(0, dataset.shape[0], 1)\n",
    "    np.random.shuffle(points)\n",
    "    added_points = [points[0]]\n",
    "\n",
    "    # Initialize the first cluster\n",
    "    clusters[0][0].append(dataset[points[0]])\n",
    "    clusters[0][1] = dataset[points[0]]\n",
    "\n",
    "    Y = -1 * np.ones(dataset.shape[0])\n",
    "\n",
    "    # Define the average scale\n",
    "    average_scale = get_average_scale(dataset)\n",
    "\n",
    "    while len(added_points) < dataset.shape[0]:\n",
    "        minimal_distance = 10e10\n",
    "        point_to_add = -1\n",
    "        for p in points:\n",
    "            if p not in added_points:\n",
    "                distance = get_distance(dataset[p], clusters[K][1])\n",
    "                if distance < minimal_distance:\n",
    "                    minimal_distance = distance\n",
    "                    point_to_add = p\n",
    "        if minimal_distance <= average_scale:\n",
    "            clusters[K][0].append(dataset[point_to_add])\n",
    "            clusters[K][1] = get_geometric_center(clusters[K][0], dim=dim)\n",
    "        else:\n",
    "            K += 1\n",
    "            clusters.append([[dataset[point_to_add]], dataset[point_to_add]])\n",
    "        added_points.append(point_to_add)\n",
    "        Y[point_to_add] = K\n",
    "\n",
    "    # Return only the clusters\n",
    "    return np.array([np.array(c[0]) for c in clusters], dtype=object)\n",
    "\n",
    "def get_cluster_radius(cluster, dim=2):\n",
    "    geometric_center = get_geometric_center(cluster, dim=dim)\n",
    "    distances = [get_distance(cluster[i], geometric_center) for i in range(cluster.shape[0])]\n",
    "    return sum(distances)/len(distances)\n",
    "\n",
    "def get_R(clusters, dim=2):\n",
    "    return sum([get_cluster_radius(clusters[i], dim=dim) for i in range(clusters.shape[0])])/clusters.shape[0]\n",
    "\n",
    "def NDRIndex(dataset, it=100, dim=2):\n",
    "    res = 0\n",
    "    for _ in range(it):\n",
    "        clusters = fill_clusters(dataset, dim=dim)\n",
    "        res += 1 - get_R(clusters, dim=dim)/get_average_scale(dataset)\n",
    "    return res/it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Testing on real data\n",
    "## Loading of data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "count = pd.read_csv('mouse.csv')\n",
    "names = list(count)[1:]\n",
    "count_cells = count[names].to_numpy().T"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "normalized_arr = preprocessing.normalize(count_cells)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "min_maxed_arr = scaler.fit_transform(count_cells)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dimension reduction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "pca2 = PCA(n_components=6)\n",
    "pca2.fit(normalized_arr)\n",
    "normalized_arr_pca = pca2.transform(normalized_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pca3 = PCA(n_components=6)\n",
    "pca3.fit(min_maxed_arr)\n",
    "min_maxed_arr_pca = pca2.transform(min_maxed_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "svd2 = TruncatedSVD(n_components=2)\n",
    "svd2.fit(normalized_arr)\n",
    "normalized_arr_svd = svd2.transform(normalized_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "svd3 = TruncatedSVD(n_components=2)\n",
    "svd3.fit(min_maxed_arr)\n",
    "min_maxed_arr_svd = svd3.transform(min_maxed_arr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDRIndex for Normalize + PCA 0.9220831144208254\n",
      "NDRIndex for MinMax + PCA 0.8358722900660471\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/gb/l9xndk8937lftyxk6hz98pth0000gn/T/ipykernel_2677/2080024294.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"NDRIndex for Normalize + PCA\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNDRIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnormalized_arr_pca\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"NDRIndex for MinMax + PCA\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNDRIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin_maxed_arr_pca\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"NDRIndex for Normalize + SVD:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNDRIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnormalized_arr_svd\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"NDRIndex for MinMax + SVD:\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNDRIndex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmin_maxed_arr_svd\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m100\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/gb/l9xndk8937lftyxk6hz98pth0000gn/T/ipykernel_2677/699498747.py\u001B[0m in \u001B[0;36mNDRIndex\u001B[0;34m(dataset, it, dim)\u001B[0m\n\u001B[1;32m     56\u001B[0m     \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     57\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0m_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 58\u001B[0;31m         \u001B[0mclusters\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfill_clusters\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     59\u001B[0m         \u001B[0mres\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mget_R\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mclusters\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdim\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mget_average_scale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mit\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/gb/l9xndk8937lftyxk6hz98pth0000gn/T/ipykernel_2677/699498747.py\u001B[0m in \u001B[0;36mfill_clusters\u001B[0;34m(dataset, dim)\u001B[0m\n\u001B[1;32m     40\u001B[0m             \u001B[0mclusters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpoint_to_add\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpoint_to_add\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0madded_points\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpoint_to_add\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m         \u001B[0mY\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpoint_to_add\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mK\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;31m# Return only the clusters\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "print(\"NDRIndex for Normalize + PCA\", NDRIndex(normalized_arr_pca[:100], dim=6))\n",
    "print(\"NDRIndex for MinMax + PCA\", NDRIndex(min_maxed_arr_pca[:100], dim=6))\n",
    "print(\"NDRIndex for Normalize + SVD:\", NDRIndex(normalized_arr_svd[:100], dim=2))\n",
    "print(\"NDRIndex for MinMax + SVD:\", NDRIndex(min_maxed_arr_svd [:100], dim=2))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clustering\n",
    "## KMeans"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for KMeans on Normalize + PCA 0.7406977052894916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cha/PycharmProjects/NDRIndex/env/lib/python3.7/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df = normalized_arr_pca[:100]\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "res = np.array(kmeans.fit_predict(df))\n",
    "arr = np.insert(df, len(df[0]), res, axis=1)\n",
    "df = arr\n",
    "df = pd.DataFrame(arr, columns = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'label'])\n",
    "score = metrics.silhouette_score(df[['C1', 'C2', 'C3', 'C4', 'C5', 'C6']], df[['label']], metric='euclidean')\n",
    "print(\"Silhouette score for KMeans on Normalize + PCA\", score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for KMeans on MinMax + PCA 0.5284162034412085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cha/PycharmProjects/NDRIndex/env/lib/python3.7/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df = min_maxed_arr_pca[:100]\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "res = np.array(kmeans.fit_predict(df))\n",
    "arr = np.insert(df, len(df[0]), res, axis=1)\n",
    "df = arr\n",
    "df = pd.DataFrame(arr, columns = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'label'])\n",
    "score = metrics.silhouette_score(df[['C1', 'C2', 'C3', 'C4', 'C5', 'C6']], df[['label']], metric='euclidean')\n",
    "print(\"Silhouette score for KMeans on MinMax + PCA\", score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for KMeans on Normalize + SVD 0.7681567169698973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cha/PycharmProjects/NDRIndex/env/lib/python3.7/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df = normalized_arr_svd[:100]\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "res = np.array(kmeans.fit_predict(df))\n",
    "arr = np.insert(df, len(df[0]), res, axis=1)\n",
    "df = arr\n",
    "df = pd.DataFrame(arr, columns = ['C1', 'C2', 'label'])\n",
    "score = metrics.silhouette_score(df[['C1', 'C2']], df[['label']], metric='euclidean')\n",
    "print(\"Silhouette score for KMeans on Normalize + SVD\", score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score for KMeans on MinMax + SVD 0.5373187834677013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cha/PycharmProjects/NDRIndex/env/lib/python3.7/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df = min_maxed_arr_svd[:100]\n",
    "kmeans = KMeans(n_clusters = 3)\n",
    "res = np.array(kmeans.fit_predict(df))\n",
    "arr = np.insert(df, len(df[0]), res, axis=1)\n",
    "df = arr\n",
    "df = pd.DataFrame(arr, columns = ['C1', 'C2', 'label'])\n",
    "score = metrics.silhouette_score(df[['C1', 'C2']], df[['label']], metric='euclidean')\n",
    "print(\"Silhouette score for KMeans on MinMax + SVD\", score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Agglomerative clustering (hclust)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cha/PycharmProjects/NDRIndex/env/lib/python3.7/site-packages/sklearn/utils/validation.py:993: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.7377217960563341"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchical_cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "df = normalized_arr_pca[:100]\n",
    "labels = hierarchical_cluster.fit_predict(df)\n",
    "arr = np.insert(df, len(df[0]), np.array(labels), axis=1)\n",
    "df = pd.DataFrame(arr, columns = ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'label'])\n",
    "metrics.silhouette_score(df[['C1', 'C2', 'C3', 'C4', 'C5', 'C6']], df[['label']], metric='euclidean')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
